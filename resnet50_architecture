digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139894768519920 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	139894768597696 [label=AddmmBackward0]
	139894768582768 -> 139894768597696
	139894873592832 [label="fc.bias
 (1000)" fillcolor=lightblue]
	139894873592832 -> 139894768582768
	139894768582768 [label=AccumulateGrad]
	139894768589392 -> 139894768597696
	139894768589392 [label=ViewBackward0]
	139894768597936 -> 139894768589392
	139894768597936 [label=MeanBackward1]
	139894768597312 -> 139894768597936
	139894768597312 [label=ReluBackward0]
	139894768596928 -> 139894768597312
	139894768596928 [label=AddBackward0]
	139894768596448 -> 139894768596928
	139894768596448 [label=NativeBatchNormBackward0]
	139894768596064 -> 139894768596448
	139894768596064 [label=ConvolutionBackward0]
	139894768595200 -> 139894768596064
	139894768595200 [label=ReluBackward0]
	139894768594624 -> 139894768595200
	139894768594624 [label=NativeBatchNormBackward0]
	139894768594384 -> 139894768594624
	139894768594384 [label=ConvolutionBackward0]
	139894768593424 -> 139894768594384
	139894768593424 [label=ReluBackward0]
	139894768592944 -> 139894768593424
	139894768592944 [label=NativeBatchNormBackward0]
	139894768592464 -> 139894768592944
	139894768592464 [label=ConvolutionBackward0]
	139894768596688 -> 139894768592464
	139894768596688 [label=ReluBackward0]
	139894768591312 -> 139894768596688
	139894768591312 [label=AddBackward0]
	139894768590832 -> 139894768591312
	139894768590832 [label=NativeBatchNormBackward0]
	139894768590256 -> 139894768590832
	139894768590256 [label=ConvolutionBackward0]
	139894768589440 -> 139894768590256
	139894768589440 [label=ReluBackward0]
	139894768588912 -> 139894768589440
	139894768588912 [label=NativeBatchNormBackward0]
	139894768588480 -> 139894768588912
	139894768588480 [label=ConvolutionBackward0]
	139894768587712 -> 139894768588480
	139894768587712 [label=ReluBackward0]
	139894768587136 -> 139894768587712
	139894768587136 [label=NativeBatchNormBackward0]
	139894768586752 -> 139894768587136
	139894768586752 [label=ConvolutionBackward0]
	139894768591072 -> 139894768586752
	139894768591072 [label=ReluBackward0]
	139894768585408 -> 139894768591072
	139894768585408 [label=AddBackward0]
	139894768585072 -> 139894768585408
	139894768585072 [label=NativeBatchNormBackward0]
	139894768584448 -> 139894768585072
	139894768584448 [label=ConvolutionBackward0]
	139894768583680 -> 139894768584448
	139894768583680 [label=ReluBackward0]
	139894768583104 -> 139894768583680
	139894768583104 [label=NativeBatchNormBackward0]
	139894768582864 -> 139894768583104
	139894768582864 [label=ConvolutionBackward0]
	139894768506032 -> 139894768582864
	139894768506032 [label=ReluBackward0]
	139894768501616 -> 139894768506032
	139894768501616 [label=NativeBatchNormBackward0]
	139894768510016 -> 139894768501616
	139894768510016 [label=ConvolutionBackward0]
	139894768516784 -> 139894768510016
	139894768516784 [label=ReluBackward0]
	139894768516208 -> 139894768516784
	139894768516208 [label=AddBackward0]
	139894768516064 -> 139894768516208
	139894768516064 [label=NativeBatchNormBackward0]
	139894768515488 -> 139894768516064
	139894768515488 [label=ConvolutionBackward0]
	139894768513280 -> 139894768515488
	139894768513280 [label=ReluBackward0]
	139894768514048 -> 139894768513280
	139894768514048 [label=NativeBatchNormBackward0]
	139894768513568 -> 139894768514048
	139894768513568 [label=ConvolutionBackward0]
	139894768512704 -> 139894768513568
	139894768512704 [label=ReluBackward0]
	139894768510400 -> 139894768512704
	139894768510400 [label=NativeBatchNormBackward0]
	139894768511888 -> 139894768510400
	139894768511888 [label=ConvolutionBackward0]
	139894768515200 -> 139894768511888
	139894768515200 [label=ReluBackward0]
	139894768508384 -> 139894768515200
	139894768508384 [label=AddBackward0]
	139894768510256 -> 139894768508384
	139894768510256 [label=NativeBatchNormBackward0]
	139894768509680 -> 139894768510256
	139894768509680 [label=ConvolutionBackward0]
	139894768508816 -> 139894768509680
	139894768508816 [label=ReluBackward0]
	139894768508240 -> 139894768508816
	139894768508240 [label=NativeBatchNormBackward0]
	139894768507856 -> 139894768508240
	139894768507856 [label=ConvolutionBackward0]
	139894768505024 -> 139894768507856
	139894768505024 [label=ReluBackward0]
	139894768506416 -> 139894768505024
	139894768506416 [label=NativeBatchNormBackward0]
	139894768506080 -> 139894768506416
	139894768506080 [label=ConvolutionBackward0]
	139894768510496 -> 139894768506080
	139894768510496 [label=ReluBackward0]
	139894768504832 -> 139894768510496
	139894768504832 [label=AddBackward0]
	139894768501376 -> 139894768504832
	139894768501376 [label=NativeBatchNormBackward0]
	139894768503824 -> 139894768501376
	139894768503824 [label=ConvolutionBackward0]
	139894768502960 -> 139894768503824
	139894768502960 [label=ReluBackward0]
	139894768502288 -> 139894768502960
	139894768502288 [label=NativeBatchNormBackward0]
	139894768502048 -> 139894768502288
	139894768502048 [label=ConvolutionBackward0]
	139894768501904 -> 139894768502048
	139894768501904 [label=ReluBackward0]
	139894768501712 -> 139894768501904
	139894768501712 [label=NativeBatchNormBackward0]
	139894768501664 -> 139894768501712
	139894768501664 [label=ConvolutionBackward0]
	139894768504592 -> 139894768501664
	139894768504592 [label=ReluBackward0]
	139894768500896 -> 139894768504592
	139894768500896 [label=AddBackward0]
	139894768500800 -> 139894768500896
	139894768500800 [label=NativeBatchNormBackward0]
	139894807489184 -> 139894768500800
	139894807489184 [label=ConvolutionBackward0]
	139894767894000 -> 139894807489184
	139894767894000 [label=ReluBackward0]
	139894767894336 -> 139894767894000
	139894767894336 [label=NativeBatchNormBackward0]
	139894767894384 -> 139894767894336
	139894767894384 [label=ConvolutionBackward0]
	139894768728192 -> 139894767894384
	139894768728192 [label=ReluBackward0]
	139894768728048 -> 139894768728192
	139894768728048 [label=NativeBatchNormBackward0]
	139894768727664 -> 139894768728048
	139894768727664 [label=ConvolutionBackward0]
	139894768501424 -> 139894768727664
	139894768501424 [label=ReluBackward0]
	139894768726656 -> 139894768501424
	139894768726656 [label=AddBackward0]
	139894768726464 -> 139894768726656
	139894768726464 [label=NativeBatchNormBackward0]
	139894768726032 -> 139894768726464
	139894768726032 [label=ConvolutionBackward0]
	139894768725312 -> 139894768726032
	139894768725312 [label=ReluBackward0]
	139894768724928 -> 139894768725312
	139894768724928 [label=NativeBatchNormBackward0]
	139894768724544 -> 139894768724928
	139894768724544 [label=ConvolutionBackward0]
	139894768723920 -> 139894768724544
	139894768723920 [label=ReluBackward0]
	139894768723440 -> 139894768723920
	139894768723440 [label=NativeBatchNormBackward0]
	139894768723152 -> 139894768723440
	139894768723152 [label=ConvolutionBackward0]
	139894768726608 -> 139894768723152
	139894768726608 [label=ReluBackward0]
	139894768722240 -> 139894768726608
	139894768722240 [label=AddBackward0]
	139894768722000 -> 139894768722240
	139894768722000 [label=NativeBatchNormBackward0]
	139894768721280 -> 139894768722000
	139894768721280 [label=ConvolutionBackward0]
	139894768720464 -> 139894768721280
	139894768720464 [label=ReluBackward0]
	139894768720128 -> 139894768720464
	139894768720128 [label=NativeBatchNormBackward0]
	139894768719696 -> 139894768720128
	139894768719696 [label=ConvolutionBackward0]
	139894768718976 -> 139894768719696
	139894768718976 [label=ReluBackward0]
	139894768718496 -> 139894768718976
	139894768718496 [label=NativeBatchNormBackward0]
	139894768718208 -> 139894768718496
	139894768718208 [label=ConvolutionBackward0]
	139894768717392 -> 139894768718208
	139894768717392 [label=ReluBackward0]
	139894768716864 -> 139894768717392
	139894768716864 [label=AddBackward0]
	139894768716528 -> 139894768716864
	139894768716528 [label=NativeBatchNormBackward0]
	139894768716048 -> 139894768716528
	139894768716048 [label=ConvolutionBackward0]
	139894768715280 -> 139894768716048
	139894768715280 [label=ReluBackward0]
	139894768714944 -> 139894768715280
	139894768714944 [label=NativeBatchNormBackward0]
	139894768714464 -> 139894768714944
	139894768714464 [label=ConvolutionBackward0]
	139894768713792 -> 139894768714464
	139894768713792 [label=ReluBackward0]
	139894768728576 -> 139894768713792
	139894768728576 [label=NativeBatchNormBackward0]
	139894768728672 -> 139894768728576
	139894768728672 [label=ConvolutionBackward0]
	139894768716624 -> 139894768728672
	139894768716624 [label=ReluBackward0]
	139894768728960 -> 139894768716624
	139894768728960 [label=AddBackward0]
	139894768729056 -> 139894768728960
	139894768729056 [label=NativeBatchNormBackward0]
	139894768729200 -> 139894768729056
	139894768729200 [label=ConvolutionBackward0]
	139894768729392 -> 139894768729200
	139894768729392 [label=ReluBackward0]
	139894768729536 -> 139894768729392
	139894768729536 [label=NativeBatchNormBackward0]
	139894768729632 -> 139894768729536
	139894768729632 [label=ConvolutionBackward0]
	139894768729824 -> 139894768729632
	139894768729824 [label=ReluBackward0]
	139894768729968 -> 139894768729824
	139894768729968 [label=NativeBatchNormBackward0]
	139894768730064 -> 139894768729968
	139894768730064 [label=ConvolutionBackward0]
	139894768729008 -> 139894768730064
	139894768729008 [label=ReluBackward0]
	139894768566576 -> 139894768729008
	139894768566576 [label=AddBackward0]
	139894768566672 -> 139894768566576
	139894768566672 [label=NativeBatchNormBackward0]
	139894768566816 -> 139894768566672
	139894768566816 [label=ConvolutionBackward0]
	139894768567008 -> 139894768566816
	139894768567008 [label=ReluBackward0]
	139894768567152 -> 139894768567008
	139894768567152 [label=NativeBatchNormBackward0]
	139894768567248 -> 139894768567152
	139894768567248 [label=ConvolutionBackward0]
	139894768567440 -> 139894768567248
	139894768567440 [label=ReluBackward0]
	139894768567584 -> 139894768567440
	139894768567584 [label=NativeBatchNormBackward0]
	139894768567680 -> 139894768567584
	139894768567680 [label=ConvolutionBackward0]
	139894768566624 -> 139894768567680
	139894768566624 [label=ReluBackward0]
	139894768567968 -> 139894768566624
	139894768567968 [label=AddBackward0]
	139894768568064 -> 139894768567968
	139894768568064 [label=NativeBatchNormBackward0]
	139894768568208 -> 139894768568064
	139894768568208 [label=ConvolutionBackward0]
	139894768568400 -> 139894768568208
	139894768568400 [label=ReluBackward0]
	139894768568544 -> 139894768568400
	139894768568544 [label=NativeBatchNormBackward0]
	139894768568640 -> 139894768568544
	139894768568640 [label=ConvolutionBackward0]
	139894768568832 -> 139894768568640
	139894768568832 [label=ReluBackward0]
	139894768568976 -> 139894768568832
	139894768568976 [label=NativeBatchNormBackward0]
	139894768569072 -> 139894768568976
	139894768569072 [label=ConvolutionBackward0]
	139894768569264 -> 139894768569072
	139894768569264 [label=ReluBackward0]
	139894768569408 -> 139894768569264
	139894768569408 [label=AddBackward0]
	139894768569504 -> 139894768569408
	139894768569504 [label=NativeBatchNormBackward0]
	139894768569648 -> 139894768569504
	139894768569648 [label=ConvolutionBackward0]
	139894768569840 -> 139894768569648
	139894768569840 [label=ReluBackward0]
	139894768569984 -> 139894768569840
	139894768569984 [label=NativeBatchNormBackward0]
	139894768570080 -> 139894768569984
	139894768570080 [label=ConvolutionBackward0]
	139894768570272 -> 139894768570080
	139894768570272 [label=ReluBackward0]
	139894768570416 -> 139894768570272
	139894768570416 [label=NativeBatchNormBackward0]
	139894768570512 -> 139894768570416
	139894768570512 [label=ConvolutionBackward0]
	139894768569456 -> 139894768570512
	139894768569456 [label=ReluBackward0]
	139894768570800 -> 139894768569456
	139894768570800 [label=AddBackward0]
	139894768570896 -> 139894768570800
	139894768570896 [label=NativeBatchNormBackward0]
	139894768571040 -> 139894768570896
	139894768571040 [label=ConvolutionBackward0]
	139894768571232 -> 139894768571040
	139894768571232 [label=ReluBackward0]
	139894768571376 -> 139894768571232
	139894768571376 [label=NativeBatchNormBackward0]
	139894768571472 -> 139894768571376
	139894768571472 [label=ConvolutionBackward0]
	139894768571664 -> 139894768571472
	139894768571664 [label=ReluBackward0]
	139894768571808 -> 139894768571664
	139894768571808 [label=NativeBatchNormBackward0]
	139894768571904 -> 139894768571808
	139894768571904 [label=ConvolutionBackward0]
	139894768570848 -> 139894768571904
	139894768570848 [label=ReluBackward0]
	139894768572192 -> 139894768570848
	139894768572192 [label=AddBackward0]
	139894768572288 -> 139894768572192
	139894768572288 [label=NativeBatchNormBackward0]
	139894768572432 -> 139894768572288
	139894768572432 [label=ConvolutionBackward0]
	139894768572624 -> 139894768572432
	139894768572624 [label=ReluBackward0]
	139894768572768 -> 139894768572624
	139894768572768 [label=NativeBatchNormBackward0]
	139894768572864 -> 139894768572768
	139894768572864 [label=ConvolutionBackward0]
	139894768573056 -> 139894768572864
	139894768573056 [label=ReluBackward0]
	139894768573200 -> 139894768573056
	139894768573200 [label=NativeBatchNormBackward0]
	139894768573296 -> 139894768573200
	139894768573296 [label=ConvolutionBackward0]
	139894768573488 -> 139894768573296
	139894768573488 [label=MaxPool2DWithIndicesBackward0]
	139894768506800 -> 139894768573488
	139894768506800 [label=ReluBackward0]
	139894768573680 -> 139894768506800
	139894768573680 [label=NativeBatchNormBackward0]
	139894768573776 -> 139894768573680
	139894768573776 [label=ConvolutionBackward0]
	139894768573968 -> 139894768573776
	139894873594272 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	139894873594272 -> 139894768573968
	139894768573968 [label=AccumulateGrad]
	139894768573728 -> 139894768573680
	139894873594592 [label="bn1.weight
 (64)" fillcolor=lightblue]
	139894873594592 -> 139894768573728
	139894768573728 [label=AccumulateGrad]
	139894768573584 -> 139894768573680
	139894873594192 [label="bn1.bias
 (64)" fillcolor=lightblue]
	139894873594192 -> 139894768573584
	139894768573584 [label=AccumulateGrad]
	139894768573440 -> 139894768573296
	139894873593152 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139894873593152 -> 139894768573440
	139894768573440 [label=AccumulateGrad]
	139894768573248 -> 139894768573200
	139894873593072 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139894873593072 -> 139894768573248
	139894768573248 [label=AccumulateGrad]
	139894768573104 -> 139894768573200
	139894873592992 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139894873592992 -> 139894768573104
	139894768573104 [label=AccumulateGrad]
	139894768573008 -> 139894768572864
	139894873592432 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139894873592432 -> 139894768573008
	139894768573008 [label=AccumulateGrad]
	139894768572816 -> 139894768572768
	139894873592672 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139894873592672 -> 139894768572816
	139894768572816 [label=AccumulateGrad]
	139894768572672 -> 139894768572768
	139894873592352 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139894873592352 -> 139894768572672
	139894768572672 [label=AccumulateGrad]
	139894768572576 -> 139894768572432
	139894873591872 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139894873591872 -> 139894768572576
	139894768572576 [label=AccumulateGrad]
	139894768572384 -> 139894768572288
	139894873591632 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	139894873591632 -> 139894768572384
	139894768572384 [label=AccumulateGrad]
	139894768572336 -> 139894768572288
	139894873591552 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	139894873591552 -> 139894768572336
	139894768572336 [label=AccumulateGrad]
	139894768572240 -> 139894768572192
	139894768572240 [label=NativeBatchNormBackward0]
	139894768573152 -> 139894768572240
	139894768573152 [label=ConvolutionBackward0]
	139894768573488 -> 139894768573152
	139894768572960 -> 139894768573152
	139894873593792 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139894873593792 -> 139894768572960
	139894768572960 [label=AccumulateGrad]
	139894768572720 -> 139894768572240
	139894873593712 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	139894873593712 -> 139894768572720
	139894768572720 [label=AccumulateGrad]
	139894768572480 -> 139894768572240
	139894873593632 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	139894873593632 -> 139894768572480
	139894768572480 [label=AccumulateGrad]
	139894768572096 -> 139894768571904
	139894873591152 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139894873591152 -> 139894768572096
	139894768572096 [label=AccumulateGrad]
	139894768571856 -> 139894768571808
	139894873591072 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139894873591072 -> 139894768571856
	139894768571856 [label=AccumulateGrad]
	139894768571712 -> 139894768571808
	139894873590992 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139894873590992 -> 139894768571712
	139894768571712 [label=AccumulateGrad]
	139894768571616 -> 139894768571472
	139894873589392 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139894873589392 -> 139894768571616
	139894768571616 [label=AccumulateGrad]
	139894768571424 -> 139894768571376
	139894873589472 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139894873589472 -> 139894768571424
	139894768571424 [label=AccumulateGrad]
	139894768571280 -> 139894768571376
	139894873596272 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139894873596272 -> 139894768571280
	139894768571280 [label=AccumulateGrad]
	139894768571184 -> 139894768571040
	139894873582848 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139894873582848 -> 139894768571184
	139894768571184 [label=AccumulateGrad]
	139894768570992 -> 139894768570896
	139894873582768 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	139894873582768 -> 139894768570992
	139894768570992 [label=AccumulateGrad]
	139894768570944 -> 139894768570896
	139894873574048 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	139894873574048 -> 139894768570944
	139894768570944 [label=AccumulateGrad]
	139894768570848 -> 139894768570800
	139894768570704 -> 139894768570512
	139894873573728 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139894873573728 -> 139894768570704
	139894768570704 [label=AccumulateGrad]
	139894768570464 -> 139894768570416
	139894873573408 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	139894873573408 -> 139894768570464
	139894768570464 [label=AccumulateGrad]
	139894768570320 -> 139894768570416
	139894873582528 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	139894873582528 -> 139894768570320
	139894768570320 [label=AccumulateGrad]
	139894768570224 -> 139894768570080
	139894873573328 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139894873573328 -> 139894768570224
	139894768570224 [label=AccumulateGrad]
	139894768570032 -> 139894768569984
	139894873582288 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	139894873582288 -> 139894768570032
	139894768570032 [label=AccumulateGrad]
	139894768569888 -> 139894768569984
	139894873573248 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	139894873573248 -> 139894768569888
	139894768569888 [label=AccumulateGrad]
	139894768569792 -> 139894768569648
	139894873572688 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139894873572688 -> 139894768569792
	139894768569792 [label=AccumulateGrad]
	139894768569600 -> 139894768569504
	139894873582048 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	139894873582048 -> 139894768569600
	139894768569600 [label=AccumulateGrad]
	139894768569552 -> 139894768569504
	139894873581968 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	139894873581968 -> 139894768569552
	139894768569552 [label=AccumulateGrad]
	139894768569456 -> 139894768569408
	139894768569216 -> 139894768569072
	139894873581488 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	139894873581488 -> 139894768569216
	139894768569216 [label=AccumulateGrad]
	139894768569024 -> 139894768568976
	139894873572128 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139894873572128 -> 139894768569024
	139894768569024 [label=AccumulateGrad]
	139894768568880 -> 139894768568976
	139894873572048 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139894873572048 -> 139894768568880
	139894768568880 [label=AccumulateGrad]
	139894768568784 -> 139894768568640
	139894873571808 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139894873571808 -> 139894768568784
	139894768568784 [label=AccumulateGrad]
	139894768568592 -> 139894768568544
	139894873571888 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139894873571888 -> 139894768568592
	139894768568592 [label=AccumulateGrad]
	139894768568448 -> 139894768568544
	139894873575968 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139894873575968 -> 139894768568448
	139894768568448 [label=AccumulateGrad]
	139894768568352 -> 139894768568208
	139894873581088 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139894873581088 -> 139894768568352
	139894768568352 [label=AccumulateGrad]
	139894768568160 -> 139894768568064
	139894873581008 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	139894873581008 -> 139894768568160
	139894768568160 [label=AccumulateGrad]
	139894768568112 -> 139894768568064
	139894873571408 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	139894873571408 -> 139894768568112
	139894768568112 [label=AccumulateGrad]
	139894768568016 -> 139894768567968
	139894768568016 [label=NativeBatchNormBackward0]
	139894768568928 -> 139894768568016
	139894768568928 [label=ConvolutionBackward0]
	139894768569264 -> 139894768568928
	139894768568736 -> 139894768568928
	139894873572368 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139894873572368 -> 139894768568736
	139894768568736 [label=AccumulateGrad]
	139894768568496 -> 139894768568016
	139894873572288 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	139894873572288 -> 139894768568496
	139894768568496 [label=AccumulateGrad]
	139894768568256 -> 139894768568016
	139894873572448 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	139894873572448 -> 139894768568256
	139894768568256 [label=AccumulateGrad]
	139894768567872 -> 139894768567680
	139894873587328 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139894873587328 -> 139894768567872
	139894768567872 [label=AccumulateGrad]
	139894768567632 -> 139894768567584
	139894873587248 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139894873587248 -> 139894768567632
	139894768567632 [label=AccumulateGrad]
	139894768567488 -> 139894768567584
	139894873580768 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139894873580768 -> 139894768567488
	139894768567488 [label=AccumulateGrad]
	139894768567392 -> 139894768567248
	139894873586928 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139894873586928 -> 139894768567392
	139894768567392 [label=AccumulateGrad]
	139894768567200 -> 139894768567152
	139894873587008 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139894873587008 -> 139894768567200
	139894768567200 [label=AccumulateGrad]
	139894768567056 -> 139894768567152
	139894873580448 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139894873580448 -> 139894768567056
	139894768567056 [label=AccumulateGrad]
	139894768566960 -> 139894768566816
	139894873586688 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139894873586688 -> 139894768566960
	139894768566960 [label=AccumulateGrad]
	139894768566768 -> 139894768566672
	139894873586608 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	139894873586608 -> 139894768566768
	139894768566768 [label=AccumulateGrad]
	139894768566720 -> 139894768566672
	139894873580128 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	139894873580128 -> 139894768566720
	139894768566720 [label=AccumulateGrad]
	139894768566624 -> 139894768566576
	139894768566480 -> 139894768730064
	139894873586368 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139894873586368 -> 139894768566480
	139894768566480 [label=AccumulateGrad]
	139894768730016 -> 139894768729968
	139894873586288 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	139894873586288 -> 139894768730016
	139894768730016 [label=AccumulateGrad]
	139894768729872 -> 139894768729968
	139894873579808 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	139894873579808 -> 139894768729872
	139894768729872 [label=AccumulateGrad]
	139894768729776 -> 139894768729632
	139894873586128 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139894873586128 -> 139894768729776
	139894768729776 [label=AccumulateGrad]
	139894768729584 -> 139894768729536
	139894873586208 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	139894873586208 -> 139894768729584
	139894768729584 [label=AccumulateGrad]
	139894768729440 -> 139894768729536
	139894873579648 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	139894873579648 -> 139894768729440
	139894768729440 [label=AccumulateGrad]
	139894768729344 -> 139894768729200
	139894873585728 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139894873585728 -> 139894768729344
	139894768729344 [label=AccumulateGrad]
	139894768729152 -> 139894768729056
	139894873585648 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	139894873585648 -> 139894768729152
	139894768729152 [label=AccumulateGrad]
	139894768729104 -> 139894768729056
	139894873579168 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	139894873579168 -> 139894768729104
	139894768729104 [label=AccumulateGrad]
	139894768729008 -> 139894768728960
	139894768728864 -> 139894768728672
	139894873578928 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139894873578928 -> 139894768728864
	139894768728864 [label=AccumulateGrad]
	139894768728624 -> 139894768728576
	139894873585408 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	139894873585408 -> 139894768728624
	139894768728624 [label=AccumulateGrad]
	139894768728480 -> 139894768728576
	139894873585328 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	139894873585328 -> 139894768728480
	139894768728480 [label=AccumulateGrad]
	139894768714032 -> 139894768714464
	139894873584928 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139894873584928 -> 139894768714032
	139894768714032 [label=AccumulateGrad]
	139894768714704 -> 139894768714944
	139894873578448 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	139894873578448 -> 139894768714704
	139894768714704 [label=AccumulateGrad]
	139894768715136 -> 139894768714944
	139894873584848 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	139894873584848 -> 139894768715136
	139894768715136 [label=AccumulateGrad]
	139894768715616 -> 139894768716048
	139894873578128 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139894873578128 -> 139894768715616
	139894768715616 [label=AccumulateGrad]
	139894768716240 -> 139894768716528
	139894873584608 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	139894873584608 -> 139894768716240
	139894768716240 [label=AccumulateGrad]
	139894768716480 -> 139894768716528
	139894873584528 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	139894873584528 -> 139894768716480
	139894768716480 [label=AccumulateGrad]
	139894768716624 -> 139894768716864
	139894768717632 -> 139894768718208
	139894873574128 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	139894873574128 -> 139894768717632
	139894768717632 [label=AccumulateGrad]
	139894768718400 -> 139894768718496
	139894873583248 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139894873583248 -> 139894768718400
	139894768718400 [label=AccumulateGrad]
	139894768718784 -> 139894768718496
	139894873583168 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139894873583168 -> 139894768718784
	139894768718784 [label=AccumulateGrad]
	139894768719168 -> 139894768719696
	139894768003472 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139894768003472 -> 139894768719168
	139894768719168 [label=AccumulateGrad]
	139894768719936 -> 139894768720128
	139894768003392 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139894768003392 -> 139894768719936
	139894768719936 [label=AccumulateGrad]
	139894768720416 -> 139894768720128
	139894768003552 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139894768003552 -> 139894768720416
	139894768720416 [label=AccumulateGrad]
	139894768720704 -> 139894768721280
	139894768003952 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139894768003952 -> 139894768720704
	139894768720704 [label=AccumulateGrad]
	139894768721424 -> 139894768722000
	139894768004032 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	139894768004032 -> 139894768721424
	139894768721424 [label=AccumulateGrad]
	139894768721616 -> 139894768722000
	139894768004112 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	139894768004112 -> 139894768721616
	139894768721616 [label=AccumulateGrad]
	139894768722144 -> 139894768722240
	139894768722144 [label=NativeBatchNormBackward0]
	139894768718736 -> 139894768722144
	139894768718736 [label=ConvolutionBackward0]
	139894768717392 -> 139894768718736
	139894768719360 -> 139894768718736
	139894873577808 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	139894873577808 -> 139894768719360
	139894768719360 [label=AccumulateGrad]
	139894768720272 -> 139894768722144
	139894873584288 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	139894873584288 -> 139894768720272
	139894768720272 [label=AccumulateGrad]
	139894768721088 -> 139894768722144
	139894873584128 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	139894873584128 -> 139894768721088
	139894768721088 [label=AccumulateGrad]
	139894768722672 -> 139894768723152
	139894768004512 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139894768004512 -> 139894768722672
	139894768722672 [label=AccumulateGrad]
	139894768723296 -> 139894768723440
	139894768004592 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139894768004592 -> 139894768723296
	139894768723296 [label=AccumulateGrad]
	139894768723728 -> 139894768723440
	139894768004672 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139894768004672 -> 139894768723728
	139894768723728 [label=AccumulateGrad]
	139894768724160 -> 139894768724544
	139894768005232 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139894768005232 -> 139894768724160
	139894768724160 [label=AccumulateGrad]
	139894768724688 -> 139894768724928
	139894768005152 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139894768005152 -> 139894768724688
	139894768724688 [label=AccumulateGrad]
	139894768725168 -> 139894768724928
	139894768005312 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139894768005312 -> 139894768725168
	139894768725168 [label=AccumulateGrad]
	139894768725504 -> 139894768726032
	139894768005792 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139894768005792 -> 139894768725504
	139894768725504 [label=AccumulateGrad]
	139894768726080 -> 139894768726464
	139894768005872 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	139894768005872 -> 139894768726080
	139894768726080 [label=AccumulateGrad]
	139894768726224 -> 139894768726464
	139894768005952 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	139894768005952 -> 139894768726224
	139894768726224 [label=AccumulateGrad]
	139894768726608 -> 139894768726656
	139894768727040 -> 139894768727664
	139894768006432 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139894768006432 -> 139894768727040
	139894768727040 [label=AccumulateGrad]
	139894768727904 -> 139894768728048
	139894768006512 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	139894768006512 -> 139894768727904
	139894768727904 [label=AccumulateGrad]
	139894768728384 -> 139894768728048
	139894768006592 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	139894768006592 -> 139894768728384
	139894768728384 [label=AccumulateGrad]
	139894768724112 -> 139894767894384
	139894768007152 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139894768007152 -> 139894768724112
	139894768724112 [label=AccumulateGrad]
	139894767893952 -> 139894767894336
	139894768007072 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	139894768007072 -> 139894767893952
	139894767893952 [label=AccumulateGrad]
	139894768714272 -> 139894767894336
	139894768007232 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	139894768007232 -> 139894768714272
	139894768714272 [label=AccumulateGrad]
	139894767894096 -> 139894807489184
	139894768007632 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139894768007632 -> 139894767894096
	139894767894096 [label=AccumulateGrad]
	139894767894192 -> 139894768500800
	139894768007712 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	139894768007712 -> 139894767894192
	139894767894192 [label=AccumulateGrad]
	139894767884208 -> 139894768500800
	139894768007792 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	139894768007792 -> 139894767884208
	139894767884208 [label=AccumulateGrad]
	139894768501424 -> 139894768500896
	139894768501136 -> 139894768501664
	139894768008272 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139894768008272 -> 139894768501136
	139894768501136 [label=AccumulateGrad]
	139894768501472 -> 139894768501712
	139894768008352 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	139894768008352 -> 139894768501472
	139894768501472 [label=AccumulateGrad]
	139894768501808 -> 139894768501712
	139894768008432 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	139894768008432 -> 139894768501808
	139894768501808 [label=AccumulateGrad]
	139894768501856 -> 139894768502048
	139894768008992 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139894768008992 -> 139894768501856
	139894768501856 [label=AccumulateGrad]
	139894768502096 -> 139894768502288
	139894768008912 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	139894768008912 -> 139894768502096
	139894768502096 [label=AccumulateGrad]
	139894768502720 -> 139894768502288
	139894768009072 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	139894768009072 -> 139894768502720
	139894768502720 [label=AccumulateGrad]
	139894768503104 -> 139894768503824
	139894768337296 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139894768337296 -> 139894768503104
	139894768503104 [label=AccumulateGrad]
	139894768504064 -> 139894768501376
	139894768337376 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	139894768337376 -> 139894768504064
	139894768504064 [label=AccumulateGrad]
	139894768501184 -> 139894768501376
	139894768337456 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	139894768337456 -> 139894768501184
	139894768501184 [label=AccumulateGrad]
	139894768504592 -> 139894768504832
	139894768505216 -> 139894768506080
	139894768337936 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139894768337936 -> 139894768505216
	139894768505216 [label=AccumulateGrad]
	139894768503488 -> 139894768506416
	139894768338016 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	139894768338016 -> 139894768503488
	139894768503488 [label=AccumulateGrad]
	139894768506896 -> 139894768506416
	139894768338096 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	139894768338096 -> 139894768506896
	139894768506896 [label=AccumulateGrad]
	139894768507136 -> 139894768507856
	139894768338656 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139894768338656 -> 139894768507136
	139894768507136 [label=AccumulateGrad]
	139894768508096 -> 139894768508240
	139894768338576 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	139894768338576 -> 139894768508096
	139894768508096 [label=AccumulateGrad]
	139894768506176 -> 139894768508240
	139894768338736 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	139894768338736 -> 139894768506176
	139894768506176 [label=AccumulateGrad]
	139894768509056 -> 139894768509680
	139894768339216 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139894768339216 -> 139894768509056
	139894768509056 [label=AccumulateGrad]
	139894768509872 -> 139894768510256
	139894768339296 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	139894768339296 -> 139894768509872
	139894768509872 [label=AccumulateGrad]
	139894768510112 -> 139894768510256
	139894768339376 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	139894768339376 -> 139894768510112
	139894768510112 [label=AccumulateGrad]
	139894768510496 -> 139894768508384
	139894768511120 -> 139894768511888
	139894768339856 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139894768339856 -> 139894768511120
	139894768511120 [label=AccumulateGrad]
	139894768510208 -> 139894768510400
	139894768339936 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	139894768339936 -> 139894768510208
	139894768510208 [label=AccumulateGrad]
	139894768510784 -> 139894768510400
	139894768340016 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	139894768340016 -> 139894768510784
	139894768510784 [label=AccumulateGrad]
	139894768512944 -> 139894768513568
	139894768340576 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139894768340576 -> 139894768512944
	139894768512944 [label=AccumulateGrad]
	139894768513808 -> 139894768514048
	139894768340496 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	139894768340496 -> 139894768513808
	139894768513808 [label=AccumulateGrad]
	139894768514432 -> 139894768514048
	139894768340656 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	139894768340656 -> 139894768514432
	139894768514432 [label=AccumulateGrad]
	139894768514864 -> 139894768515488
	139894768341136 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139894768341136 -> 139894768514864
	139894768514864 [label=AccumulateGrad]
	139894768514624 -> 139894768516064
	139894768341216 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	139894768341216 -> 139894768514624
	139894768514624 [label=AccumulateGrad]
	139894768515920 -> 139894768516064
	139894768341296 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	139894768341296 -> 139894768515920
	139894768515920 [label=AccumulateGrad]
	139894768515200 -> 139894768516208
	139894768516736 -> 139894768510016
	139894768342496 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	139894768342496 -> 139894768516736
	139894768516736 [label=AccumulateGrad]
	139894768515008 -> 139894768501616
	139894768342576 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139894768342576 -> 139894768515008
	139894768515008 [label=AccumulateGrad]
	139894768504976 -> 139894768501616
	139894768342656 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139894768342656 -> 139894768504976
	139894768504976 [label=AccumulateGrad]
	139894768508768 -> 139894768582864
	139894768343216 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139894768343216 -> 139894768508768
	139894768508768 [label=AccumulateGrad]
	139894768583488 -> 139894768583104
	139894768343136 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139894768343136 -> 139894768583488
	139894768583488 [label=AccumulateGrad]
	139894768506848 -> 139894768583104
	139894768343296 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139894768343296 -> 139894768506848
	139894768506848 [label=AccumulateGrad]
	139894768583920 -> 139894768584448
	139894768343776 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139894768343776 -> 139894768583920
	139894768583920 [label=AccumulateGrad]
	139894768584688 -> 139894768585072
	139894768343856 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	139894768343856 -> 139894768584688
	139894768584688 [label=AccumulateGrad]
	139894768584832 -> 139894768585072
	139894768343936 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	139894768343936 -> 139894768584832
	139894768584832 [label=AccumulateGrad]
	139894768585312 -> 139894768585408
	139894768585312 [label=NativeBatchNormBackward0]
	139894768583344 -> 139894768585312
	139894768583344 [label=ConvolutionBackward0]
	139894768516784 -> 139894768583344
	139894768515440 -> 139894768583344
	139894768341776 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	139894768341776 -> 139894768515440
	139894768515440 [label=AccumulateGrad]
	139894768584064 -> 139894768585312
	139894768341856 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	139894768341856 -> 139894768584064
	139894768584064 [label=AccumulateGrad]
	139894768584304 -> 139894768585312
	139894768341936 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	139894768341936 -> 139894768584304
	139894768584304 [label=AccumulateGrad]
	139894768585888 -> 139894768586752
	139894768344336 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139894768344336 -> 139894768585888
	139894768585888 [label=AccumulateGrad]
	139894768586992 -> 139894768587136
	139894768344416 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139894768344416 -> 139894768586992
	139894768586992 [label=AccumulateGrad]
	139894768587472 -> 139894768587136
	139894768344496 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139894768344496 -> 139894768587472
	139894768587472 [label=AccumulateGrad]
	139894768587952 -> 139894768588480
	139894768345056 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139894768345056 -> 139894768587952
	139894768587952 [label=AccumulateGrad]
	139894768588672 -> 139894768588912
	139894768344976 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139894768344976 -> 139894768588672
	139894768588672 [label=AccumulateGrad]
	139894768589296 -> 139894768588912
	139894768345136 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139894768345136 -> 139894768589296
	139894768589296 [label=AccumulateGrad]
	139894768589632 -> 139894768590256
	139894768345616 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139894768345616 -> 139894768589632
	139894768589632 [label=AccumulateGrad]
	139894768590400 -> 139894768590832
	139894768345696 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	139894768345696 -> 139894768590400
	139894768590400 [label=AccumulateGrad]
	139894768590592 -> 139894768590832
	139894768345776 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	139894768345776 -> 139894768590592
	139894768590592 [label=AccumulateGrad]
	139894768591072 -> 139894768591312
	139894768591696 -> 139894768592464
	139894768346256 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139894768346256 -> 139894768591696
	139894768591696 [label=AccumulateGrad]
	139894768592704 -> 139894768592944
	139894768346336 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	139894768346336 -> 139894768592704
	139894768592704 [label=AccumulateGrad]
	139894768593280 -> 139894768592944
	139894768346416 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	139894768346416 -> 139894768593280
	139894768593280 [label=AccumulateGrad]
	139894768593664 -> 139894768594384
	139894768346976 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139894768346976 -> 139894768593664
	139894768593664 [label=AccumulateGrad]
	139894768594432 -> 139894768594624
	139894768346896 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	139894768346896 -> 139894768594432
	139894768594432 [label=AccumulateGrad]
	139894768595008 -> 139894768594624
	139894768347056 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	139894768347056 -> 139894768595008
	139894768595008 [label=AccumulateGrad]
	139894768595344 -> 139894768596064
	139894768347536 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139894768347536 -> 139894768595344
	139894768595344 [label=AccumulateGrad]
	139894768596304 -> 139894768596448
	139894768347616 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	139894768347616 -> 139894768596304
	139894768596304 [label=AccumulateGrad]
	139894768583296 -> 139894768596448
	139894768347696 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	139894768347696 -> 139894768583296
	139894768583296 [label=AccumulateGrad]
	139894768596688 -> 139894768596928
	139894768597456 -> 139894768597696
	139894768597456 [label=TBackward0]
	139894768598176 -> 139894768597456
	139894805759392 [label="fc.weight
 (1000, 2048)" fillcolor=lightblue]
	139894805759392 -> 139894768598176
	139894768598176 [label=AccumulateGrad]
	139894768597696 -> 139894768519920
}
